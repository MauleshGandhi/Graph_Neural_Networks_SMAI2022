{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "77de8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv,GCNConv\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool,global_max_pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba1bb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='.', name='PROTEINS').shuffle()\n",
    "\n",
    "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
    "test_dataset  = dataset[int(len(dataset)*0.8):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3d761fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_mean(torch.nn.Module):\n",
    "    def __init__(self, dim_h, num_layers):\n",
    "        super(GNN_mean, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(GCNConv(dataset.num_node_features, dim_h))\n",
    "        for i in range(1,num_layers):\n",
    "            self.layers.append(GCNConv(dim_h, dim_h))\n",
    "        \n",
    "        self.lin = Linear(dim_h, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch,num_layers):\n",
    "        \n",
    "        h = x\n",
    "        for i in range(num_layers):\n",
    "            h = (self.layers[i](h, edge_index)).relu()\n",
    "                \n",
    "\n",
    "        hG = global_mean_pool(h, batch)\n",
    "\n",
    "        h = F.dropout(hG, p=0.5, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        return hG, F.log_softmax(h, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8ed660c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_max(torch.nn.Module):\n",
    "    def __init__(self, dim_h, num_layers):\n",
    "        super(GNN_max, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(GCNConv(dataset.num_node_features, dim_h))\n",
    "        for i in range(1,num_layers):\n",
    "            self.layers.append(GCNConv(dim_h, dim_h))\n",
    "        \n",
    "        self.lin = Linear(dim_h, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch,num_layers):\n",
    "        \n",
    "        h = x\n",
    "        for i in range(num_layers):\n",
    "            h = (self.layers[i](h, edge_index)).relu()\n",
    "                \n",
    "\n",
    "        hG = global_max_pool(h, batch)\n",
    "\n",
    "        h = F.dropout(hG, p=0.5, training=self.training)\n",
    "        h = self.lin(h)\n",
    "        \n",
    "        return hG, F.log_softmax(h, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5cd91776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dim_h, num_layers):\n",
    "        super(GIN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(GINConv(Sequential(Linear(dataset.num_node_features, dim_h),\n",
    "                           BatchNorm1d(dim_h), ReLU(),\n",
    "                           Linear(dim_h, dim_h), ReLU())))\n",
    "        for i in range(1,num_layers):\n",
    "            self.layers.append(GINConv(Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "                               Linear(dim_h, dim_h), ReLU())))\n",
    "            \n",
    "        self.fin = Linear(dim_h*num_layers, dim_h*num_layers)\n",
    "        self.out = Linear(dim_h*num_layers, dataset.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch, num_layers):\n",
    "        h = x\n",
    "        for i in range(num_layers):\n",
    "            h = self.layers[i](h, edge_index) \n",
    "            h_sum = global_add_pool(h, batch)\n",
    "            if(i == 0):\n",
    "                h_fin = h_sum\n",
    "            else:\n",
    "                h_fin = torch.cat((h_fin,h_sum), dim=1)\n",
    "\n",
    "\n",
    "        h_fin = (self.fin(h_fin)).relu()\n",
    "        h_fin = F.dropout(h_fin, p=0.5, training=self.training)\n",
    "        h_fin = self.out(h_fin)\n",
    "        \n",
    "        return h_fin, F.log_softmax(h_fin, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d953907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader,num_layers):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=0.01)\n",
    "    epochs = 100\n",
    "    train_acc=[]\n",
    "    model.train()\n",
    "    for epoch in range(epochs+1):\n",
    "        acc = 0\n",
    "        tot = 0\n",
    "\n",
    "        for data in loader:\n",
    "            optimizer.zero_grad()\n",
    "            _, out = model(data.x, data.edge_index, data.batch, num_layers)\n",
    "            loss = criterion(out, data.y)\n",
    "            acc += (out.argmax(dim=1) == data.y).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot = tot + len(data.y)\n",
    "        train_acc.append(acc/(tot))\n",
    "\n",
    "    print(train_acc)\n",
    "    test_acc = test(model, test_loader,num_layers)\n",
    "    print(test_acc)\n",
    "    \n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, num_layers):\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    tot = 0\n",
    "\n",
    "    for data in loader:\n",
    "        _, out = model(data.x, data.edge_index, data.batch, num_layers)\n",
    "        acc += (out.argmax(dim=1) == data.y).sum()\n",
    "        tot = tot+len(data.y)\n",
    "    acc = acc/tot\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1a51c40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.5640), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910)]\n",
      "tensor(0.6143)\n",
      "[tensor(0.5584), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910), tensor(0.5910)]\n",
      "tensor(0.6143)\n",
      "[tensor(0.5472), tensor(0.6326), tensor(0.6528), tensor(0.6831), tensor(0.7045), tensor(0.7169), tensor(0.7000), tensor(0.7303), tensor(0.7157), tensor(0.7360), tensor(0.6876), tensor(0.7213), tensor(0.7225), tensor(0.7292), tensor(0.7360), tensor(0.7371), tensor(0.7315), tensor(0.7022), tensor(0.7404), tensor(0.7011), tensor(0.7157), tensor(0.7416), tensor(0.7360), tensor(0.7281), tensor(0.7506), tensor(0.7539), tensor(0.7270), tensor(0.7202), tensor(0.7101), tensor(0.7202), tensor(0.7371), tensor(0.7169), tensor(0.7315), tensor(0.7202), tensor(0.7393), tensor(0.7393), tensor(0.7461), tensor(0.7337), tensor(0.7393), tensor(0.7494), tensor(0.7382), tensor(0.7393), tensor(0.7404), tensor(0.7225), tensor(0.7258), tensor(0.7494), tensor(0.7393), tensor(0.7506), tensor(0.7449), tensor(0.7494), tensor(0.7494), tensor(0.7326), tensor(0.7472), tensor(0.7382), tensor(0.7180), tensor(0.7393), tensor(0.7472), tensor(0.7371), tensor(0.7449), tensor(0.7494), tensor(0.7303), tensor(0.7438), tensor(0.7438), tensor(0.7382), tensor(0.7584), tensor(0.7461), tensor(0.7337), tensor(0.7371), tensor(0.7584), tensor(0.7472), tensor(0.7494), tensor(0.7517), tensor(0.7270), tensor(0.7281), tensor(0.7135), tensor(0.7292), tensor(0.7483), tensor(0.7528), tensor(0.7584), tensor(0.7292), tensor(0.7607), tensor(0.7483), tensor(0.7607), tensor(0.7348), tensor(0.7292), tensor(0.7573), tensor(0.7382), tensor(0.7427), tensor(0.7348), tensor(0.7562), tensor(0.7404), tensor(0.7449), tensor(0.7315), tensor(0.7461), tensor(0.7528), tensor(0.7427), tensor(0.7596), tensor(0.7427), tensor(0.7202), tensor(0.7562), tensor(0.7562)]\n",
      "tensor(0.7534)\n"
     ]
    }
   ],
   "source": [
    "num_layers = 5\n",
    "\n",
    "gnn_max = GNN_max(dim_h=32,num_layers=5)\n",
    "gnn_mean = GNN_mean(dim_h=32,num_layers=5)\n",
    "gin = GIN(dim_h=32, num_layers=5)\n",
    "\n",
    "gnn_max = train(gnn_max, train_loader, num_layers)\n",
    "gnn_mean = train(gnn_mean, train_loader, num_layers)\n",
    "gin = train(gin, train_loader, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa2ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
